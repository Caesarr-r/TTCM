# TTCM: Towards Effective Few-Shot OOD Detection for Text-Attributed Graphs via Topology-Text Consensus Modeling

This repository contains the implementation of **TTCM**, a framework for few-shot graph node Out-of-Distribution (OOD) detection and classification.

## Core Ideas

- **Consensus-Gated EM (CGEM)**: An EM-style training process where the E-step estimates reliability `r` with DARE, and the M-step uses `r` to gate the training process, effectively reducing the FPR95.
- **Structure-Semantic Dual Augmentation (SSDA)**: Augments supervision from both structural (S1) and semantic (S2) perspectives, and statically generates OOD concepts.
- **DARE**: An explicit inference engine that performs a triangular consistency check (graph-text, graph-prototype, text-prototype) to produce a reliability score `r`.
---

## 1. Project Structure

```
TTCM/
├── main.py                         # Main entry point
├── src/
│   ├── config.py                   # Default configurations
│   ├── models/                     # Encoders and DARE modules
│   ├── data/                       # Data loading and preprocessing
│   ├── train/                      # Training logic (CGEM)
│   ├── eval/                       # Evaluation logic
│   ├── pretrain/                   # Pre-training scripts
│   └── utils/                      # Utilities (tokenizer, metrics, etc.)
└── (External directories, ignored by .gitignore)
    ├── dataset/                    # processed graph data (.pt)
    ├── res/                        # Pre-trained weights (.pkl)
    └── llm_cache/                  # LLM candidate cache (.json)
```
---


---

## 2. Quick Start

### a. Reproducibility with Cora

This repository includes the raw data file and LLM-generated candidate caches for the **Cora** dataset. To reproduce the reported results, you need to preprocess the data and generate the pre-trained model weights.

The following files/directories are now tracked by Git:

- `dataset/cora/cora.pt`: Contains the raw graph data for Cora (with original text features). This file needs to be preprocessed to generate `cora_processed.pt` before use.
- `llm_cache/`: Contains cached OOD concepts generated by the LLM for multiple datasets (Cora, CiteSeer, PubMed, etc.).

**Note**: 
- Pre-trained model weights (`res/`) are not included in this repository. 
- The processed data file (`cora_processed.pt`) is not included. You need to generate both using the pre-training scripts (see section b below).

### b. Prepare Data and Weights for Datasets

For **Cora** or other datasets, you need to generate the processed data file and pre-trained model weights. The preprocessing pipeline will:
1. Load the raw data file (`<name>.pt`) and preprocess node text features using pre-trained sentence embeddings for dimension alignment
2. Generate the processed data file (`<name>_processed.pt`)
3. Pre-train the model and save weights to `res/<name>/pretrained_model.pkl`

**For Cora dataset:**
```bash
python src/pretrain/prepare_pretrain.py \
  --data_name cora --gpu 0
```

**For other datasets**, place your raw data file at `dataset/<name>/<name>.pt` first, then run:
```bash
# Example for a new dataset named 'my_dataset'
python src/pretrain/prepare_pretrain.py \
  --data_name my_dataset --gpu 0
```

### c. Run a Single Experiment

```bash
python main.py --gpu 0 --use_llm_cache \
  --data_name cora --n_way 3 --k_shot 5 --seed 13 \
  --tau_quantile 0.3 --clamp_min 0.02 \
  --lambda_clean 0.5 --lambda_push 0.2 --lambda_align 0.08 \
  --pseudo_k_neighbors 3 --s1_threshold 0.85 \
  --see_samples_per_class 8 --s2_threshold 0.35 \
  --ft_lr 1e-5 --dare_lr 1e-3
```
